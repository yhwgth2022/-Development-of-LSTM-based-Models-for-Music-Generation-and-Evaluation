{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIgSGWc6BK7H",
    "outputId": "74b7fdb0-dad4-4ec7-da73-3a2abffff864"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -2o (c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -2o (c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -2o (c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -2o (c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -2o (c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -2o (c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pretty_midi in d:\\pythonanaconda\\envs\\tf\\lib\\site-packages (0.2.10)\n",
      "Requirement already satisfied: numpy>=1.7.0 in d:\\pythonanaconda\\envs\\tf\\lib\\site-packages (from pretty_midi) (1.23.5)\n",
      "Requirement already satisfied: mido>=1.1.16 in d:\\pythonanaconda\\envs\\tf\\lib\\site-packages (from pretty_midi) (1.3.0)\n",
      "Requirement already satisfied: six in d:\\pythonanaconda\\envs\\tf\\lib\\site-packages (from pretty_midi) (1.16.0)\n",
      "Requirement already satisfied: packaging~=23.1 in d:\\pythonanaconda\\envs\\tf\\lib\\site-packages (from mido>=1.1.16->pretty_midi) (23.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pretty_midi\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pretty_midi\n",
    "import glob\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import ceil\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "\n",
    "\n",
    "velo_inc = 5\n",
    "dim = 128*2 + 100 + int(ceil(126/velo_inc))  \n",
    "class Event:\n",
    "    def __init__(self, s, t, v):\n",
    "        self.time = s\n",
    "        self.type = t\n",
    "        self.val = v\n",
    "    def encode(self):\n",
    "        if self.type == 'down':\n",
    "            return self.val\n",
    "        elif self.type == 'up':\n",
    "            return 128 + self.val\n",
    "        elif self.type == 'shift':\n",
    "            return 128*2 + self.val\n",
    "        else:\n",
    "            return 128*2 + 100 + self.val\n",
    "    @staticmethod\n",
    "    def decode(code):\n",
    "        if code < 128:\n",
    "            return 'down', code\n",
    "        elif code < 128*2:\n",
    "            return 'up', code - 128\n",
    "        elif code < 128*2 + 100:\n",
    "            return 'shift', (code - 128*2)/100 + 0.01\n",
    "        else:\n",
    "            return 'velo', (code - 128*2 - 100)*velo_inc + int(velo_inc/2)\n",
    "        \n",
    "def piano2seq(midi):\n",
    "    '''\n",
    "    Convert a midi object to a sequence of events\n",
    "    :param midi: midi object or the file name of the midi file\n",
    "    :return: numpy array that contains the sequence of events\n",
    "    '''\n",
    "    if type(midi) is str:\n",
    "        midi = pretty_midi.PrettyMIDI(midi)\n",
    "    piano = midi.instruments[0]\n",
    "    velo = 0\n",
    "    q = []\n",
    "    for note in piano.notes:\n",
    "        if note.velocity != velo:\n",
    "            q.append(Event(note.start, 'velo', int(min(note.velocity, 125)/velo_inc)))\n",
    "            velo = note.velocity\n",
    "        q.append(Event(note.start, 'down', note.pitch))\n",
    "        q.append(Event(note.end, 'up', note.pitch))\n",
    "    t = 0\n",
    "    qfull = []\n",
    "    for e in sorted(q, key=lambda x: x.time):\n",
    "        d = e.time - t\n",
    "        while d > 0.01:\n",
    "            dd = min(d, 1) - 0.01\n",
    "            qfull.append(Event(t, 'shift', int(dd*100)))\n",
    "            d = d - dd\n",
    "        t = e.time\n",
    "        qfull.append(e)\n",
    "    seq = np.zeros((len(qfull),), dtype=np.int32)\n",
    "    for i, e in enumerate(qfull):\n",
    "        seq[i] = e.encode()\n",
    "    assert np.max(seq) < dim\n",
    "    return seq\n",
    "\n",
    "def seq2piano(seq):\n",
    "    '''\n",
    "    Convert a sequence of events to midi\n",
    "    :param seq: numpy array that contains the sequence\n",
    "    :return: midi object\n",
    "    '''\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    piano = pretty_midi.Instrument(program=0, is_drum=False, name='piano')\n",
    "    midi.instruments.append(piano)\n",
    "\n",
    "    if seq.ndim > 1:\n",
    "        seq = np.argmax(seq, axis=-1)\n",
    "    inote = {}\n",
    "    velo = 40\n",
    "    time = 0.\n",
    "    for e in seq:\n",
    "        t, v = Event.decode(e)\n",
    "        if t == 'shift':\n",
    "            time += v\n",
    "        elif t == 'velo':\n",
    "            velo = v\n",
    "            for n in inote.values():\n",
    "                if n[2] == time:\n",
    "                    n[0] = v\n",
    "        elif t == 'down':\n",
    "            n = inote.get(v, None)\n",
    "            if n is not None:\n",
    "                logging.debug('consecutive downs for pitch %d at time %d and %d' % (v, n[2], time))\n",
    "            else:\n",
    "                inote[v]  = [velo, v, time, -1]\n",
    "        else:\n",
    "            n = inote.get(v, None)\n",
    "            if n is not None:\n",
    "                n[-1] = time\n",
    "                if n[-1] > n[-2]:\n",
    "                    piano.notes.append(pretty_midi.Note(*n))\n",
    "                else:\n",
    "                    logging.debug('note with non-positive duration for pitch %d at time %d' % (n[1], n[2]))\n",
    "                del inote[v]\n",
    "            else:\n",
    "                logging.debug('up without down for pitch %d at time %d' % (v, time))\n",
    "    # clean out the incomplete note buffer, assuming these note end at last\n",
    "    for n in inote.values():\n",
    "        n[-1] = time\n",
    "        if n[-1] > n[-2]:\n",
    "            piano.notes.append(pretty_midi.Note(*n))\n",
    "    return midi\n",
    "\n",
    "def segment(seq, maxlen=150):\n",
    "    assert len(seq) > maxlen\n",
    "    inc = int(maxlen/2)\n",
    "    i = inc\n",
    "    t = np.ones((maxlen+1,), dtype=np.int32)\n",
    "    t[0] = (128*2+1)\n",
    "    t[1:] = seq[:maxlen]\n",
    "    s = [t]\n",
    "    while i+maxlen+1 < len(seq):\n",
    "        s.append(seq[i:i+maxlen+1])\n",
    "        i += inc\n",
    "    return np.stack(s, axis=0)\n",
    "\n",
    "def process_midi_seq(all_midis=None, datadir='data', n=10000, maxlen=150):\n",
    "    '''\n",
    "    Process a list of midis, convert them to sequences and segment sequences into segments of length max_len\n",
    "    :param all_midis: the list of midis. If None, midis will be loaded from files\n",
    "    :param datadir: data directory, assume under this directory, we have the \"maestro-v1.0.0\" midi directory\n",
    "    :param n: # of segments to return\n",
    "    :param maxlen: the length of the segments\n",
    "    :return: numpy array of shape [n', max_len] for the segments. n' tries to be close to n but may not be exactly n.\n",
    "    '''\n",
    "    if all_midis is None:\n",
    "        all_midis = glob.glob(datadir+'/maestro-v1.0.0/**/*.midi')\n",
    "        random.seed()    # for debug purpose, you can pass a fix number when calling seed()\n",
    "        random.shuffle(all_midis)\n",
    "    data = []\n",
    "    k = 0\n",
    "    for m in all_midis:\n",
    "        seq = segment(piano2seq(m), maxlen)\n",
    "        data.append(seq)\n",
    "        k += len(seq)\n",
    "        if k > n:\n",
    "            break\n",
    "    return np.vstack(data)\n",
    "\n",
    "def random_piano(n=100):\n",
    "    '''\n",
    "    Generate random piano note\n",
    "    :param n: # of notes to be generated\n",
    "    :return: midi object with the notes\n",
    "    '''\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    piano = pretty_midi.Instrument(program=0, is_drum=False, name='piano')\n",
    "    midi.instruments.append(piano)\n",
    "\n",
    "    pitchs = np.random.choice(128, size=n)\n",
    "    velos = np.random.choice(np.arange(10, 80), size=n)\n",
    "    durations = np.abs(np.random.randn(n) + 1)\n",
    "    intervs = np.abs(0.2*np.random.randn(n) + 0.3)\n",
    "    time = 0.5\n",
    "    for i in range(n):\n",
    "        piano.notes.append(pretty_midi.Note(velos[i], pitchs[i], time, time+durations[i]))\n",
    "        time += intervs[i]\n",
    "    return midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1\n",
    "This code segment demonstrates the application of a deep learning model, Critic, to solve the binary classification problem of musical sequences. The goal is to differentiate between \"good music\" and \"bad music\", essentially judging the quality of music based on its sequential data.\n",
    "\n",
    "## Model Design: \n",
    "\n",
    "Implemented a LSTM (Long Short-Term Memory) neural network model named Critic for processing sequence data. The model includes an LSTM layer and a fully connected layer, along with a BCEWithLogitsLoss loss function for binary classification. \n",
    "\n",
    "Data Preparation: Transformed good and bad music data into tensors and assigned corresponding labels (1 for good music, 0 for bad music). The dataset was then split into training, validation, and test sets. \n",
    "\n",
    "Training Process: The model was trained on the training set, including forward propagation, loss calculation, backpropagation, and parameter updates. \n",
    "\n",
    "Validation Process: Evaluated the model's performance on the validation set, calculating validation loss and accuracy. \n",
    "\n",
    "Testing Process: Conducted a final performance assessment on the test set, using the model to predict each sequence and calculating the overall accuracy. Results Obtained:\n",
    "\n",
    "On the test set, the model successfully made correct predictions for 1535 out of 2029 samples, achieving a classification accuracy of 75.65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v-ZmZIktZm6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Starting new HTTPS connection (1): storage.googleapis.com:443\n",
      "DEBUG:https://storage.googleapis.com:443 \"GET /magentadata/datasets/maestro/v1.0.0/maestro-v1.0.0-midi.zip HTTP/1.1\" 200 46579421\n"
     ]
    }
   ],
   "source": [
    "# Import midi data\n",
    "data_url = \"https://storage.googleapis.com/magentadata/datasets/maestro/v1.0.0/maestro-v1.0.0-midi.zip\"\n",
    "data_path = \"maestro-v1.0.0-midi.zip\"\n",
    "response = requests.get(data_url, stream=True)\n",
    "with open(data_path, 'wb') as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        f.write(chunk)\n",
    "        \n",
    "with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    \n",
    "os.remove(data_path)\n",
    "good_sequences = glob.glob('maestro-v1.0.0/**/*.midi', recursive=True)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    This code defines a class named Critic, which is a neural network model for binary classification tasks. \n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights=None):\n",
    "        super(Critic, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=151, hidden_size=100, num_layers=3,\n",
    "                    batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(100, 1)  \n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This method defines how the input data x passes through the network.\n",
    "        The input is first fed through the LSTM layer. The output of the LSTM layer is then processed to only keep \n",
    "        the last time step's output.\n",
    "        This output is then passed through the fully connected layer to produce the final output.\n",
    "        \"\"\"\n",
    "        out, _ = self.lstm(x)\n",
    "        if len(out.shape) == 3:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out \n",
    "    def train_model1(self, x, label):\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.forward(x)\n",
    "        loss = self.criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    def score(self, x):\n",
    "        with torch.no_grad():\n",
    "            return torch.sigmoid(self.forward(x)).cpu().numpy() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "onfQtdezZQVS"
   },
   "outputs": [],
   "source": [
    "good_music = process_midi_seq(good_sequences)\n",
    "# Generate bad music and convert them to sequences\n",
    "bad_sequences = []\n",
    "midi = random_piano(40000)  \n",
    "bad_sequences = process_midi_seq([midi])  # This function should convert midi to a sequence of events of shape (51,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eqw8IyS1obFY"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code segment is preparing and splitting a dataset for training, validation, and testing.\n",
    "Labels for the sequences: 0 for bad music (bad_labels) and 1 for good music (good_labels).\n",
    "Allocates 70% of the dataset for training (train_size), and the remaining 30% is evenly divided between validation and testing.\n",
    "\"\"\"\n",
    "bad_labels = torch.zeros(len(bad_sequences), 1)\n",
    "good_labels = torch.ones(len(good_music), 1)\n",
    "bad_music_tensor = torch.tensor(bad_sequences)\n",
    "good_music_tensor = torch.tensor(good_music)\n",
    "all_data = torch.cat([bad_music_tensor, good_music_tensor], 0)\n",
    "all_labels = torch.cat([bad_labels, good_labels], dim=0)\n",
    "dataset = TensorDataset(all_data, all_labels)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.7 * len(dataset))  # 70% of the dataset for training\n",
    "remaining_size = len(dataset) - train_size\n",
    "valid_size = int(0.5 * remaining_size)  # Split the remaining 30% equally for validation and test\n",
    "test_size = remaining_size - valid_size\n",
    "# Split the dataset\n",
    "train_dataset, remaining_dataset = random_split(dataset, [train_size, remaining_size])\n",
    "valid_dataset, test_dataset = random_split(remaining_dataset, [valid_size, test_size])\n",
    "# Create dataloaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWnmY6zOobIe",
    "outputId": "11ae28e0-045d-4874-e5fa-f3f38c51bd7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss: 0.5504, Validation Loss: 0.5547, Validation Accuracy: 0.7520\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The training and validation process of Critic \n",
    "\"\"\"\n",
    "model1 = Critic()\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()  # Set the model to training mode\n",
    "    total_train_loss = 0.0\n",
    "    # Training\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.float()\n",
    "        batch_labels = batch_labels.float()\n",
    "        loss = model1.train_model1(batch_data, batch_labels)\n",
    "        total_train_loss += loss\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    # Validation\n",
    "    model1.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in valid_loader:\n",
    "            batch_data = batch_data.float()\n",
    "            batch_labels = batch_labels.float()\n",
    "            outputs = model1(batch_data)\n",
    "            val_loss = model1.criterion(outputs, batch_labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_val_correct += (predicted == batch_labels).sum().item()\n",
    "    average_val_loss = total_val_loss / len(valid_loader)\n",
    "    val_accuracy = total_val_correct / (len(valid_loader.dataset))\n",
    "    if epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb0G4anHobNJ",
    "outputId": "58ec5c31-97dd-4e8b-d5a5-38b92a37634f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1535\n",
      "Total predictions: 2029\n",
      "The classification accuracy on the test set is: 75.65%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# Iterate over batches in the test_loader\n",
    "for batch_data, batch_labels in test_loader:\n",
    "    batch_data = batch_data.float()\n",
    "    scores = model1.forward(batch_data)\n",
    "    # Convert scores to binary predictions (good or bad)\n",
    "    predicted = (scores > 0.8).float().squeeze()\n",
    "    # Convert labels to float for comparison (if they are not already)\n",
    "    batch_labels = batch_labels.float().squeeze()\n",
    "    # Count the number of correct predictions\n",
    "    correct += (predicted == batch_labels).sum().item()\n",
    "    total += batch_labels.size(0)\n",
    "\n",
    "# Print values of correct and total\n",
    "print(\"Correct predictions:\", correct)\n",
    "print(\"Total predictions:\", total)\n",
    "# Compute accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"The classification accuracy on the test set is: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqmVthb3xaFG"
   },
   "source": [
    "# Task2:\n",
    "\n",
    "The code for the Composer model aims to solve the problem of automatic music generation. \n",
    "The model is designed to generate musical sequences based on input data, representing a creative application of deep learning in the field of music.\n",
    "\n",
    "## Model Architecture: \n",
    "\n",
    "Implemented a Composer class, an LSTM-based neural network model, which is well-suited for sequential data like music. \n",
    "\n",
    "The model includes: A multi-layer LSTM network for processing sequences. A fully connected layer for output generation. The forward pass method for defining the data flow through the model. A method for initializing the LSTM's hidden and cell states. A compose method for generating music sequences based on a starting note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSH0xwC8UNGn",
    "outputId": "730a3365-6c51-4002-d529-4a76962d2981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 151])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\"\"\"\n",
    "Creating and processing a batch of data from the good_music_tensor using PyTorch's DataLoader.\n",
    "\"\"\"\n",
    "good_music_tensor = torch.tensor(good_music, dtype=torch.long)\n",
    "# Create Dataset and DataLoader\n",
    "data_loader = DataLoader(good_music_tensor, batch_size=64, shuffle=True)\n",
    "# Iterate over the DataLoader to get a single batch\n",
    "for batch in data_loader:\n",
    "    x= batch  \n",
    "    print(x.shape)  \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vXPgMvngUNJB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code defines a neural network model named Composer, which is designed for generating musical sequences. \n",
    "The model is built using PyTorch.\n",
    "\"\"\"\n",
    "def one_hot_encode(note, num_notes):\n",
    "    \"\"\"\n",
    "     converts a musical note into a one-hot encoded tensor. \n",
    "     This is used for representing discrete elements (like notes) in a format suitable for neural network processing.\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(num_notes, dtype=torch.float)\n",
    "    tensor[note] = 1.0\n",
    "    return tensor\n",
    "\n",
    "class Composer(nn.Module):\n",
    "    \"\"\"\n",
    "    this Composer model is a neural network designed to generate music sequences. \n",
    "    It uses LSTM layers to capture the temporal dependencies in music data and predicts one note at a time, \n",
    "    building a sequence iteratively.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, lstm_units, num_layers=2):\n",
    "        super(Composer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.num_layers = num_layers \n",
    "        self.lstm = nn.LSTM(input_dim, lstm_units, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_units, input_dim)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "      state_h, state_c = prev_state\n",
    "      output, state = self.lstm(x, prev_state)\n",
    "      logits = self.fc(output)\n",
    "      return logits, state\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        # Initialize the hidden and cell state to zeros\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.lstm_units),\n",
    "                torch.zeros(self.num_layers, batch_size, self.lstm_units))\n",
    "    \n",
    "    def compose(self, start_sequence, length, temperature=1.0):\n",
    "        # Ensure that start_sequence is a torch tensor\n",
    "        if not isinstance(start_sequence, torch.Tensor):\n",
    "            start_sequence = torch.tensor(start_sequence, dtype=torch.long)\n",
    "        # Initialize the hidden state\n",
    "        state_h, state_c = self.init_state(1)  # Batch size of 1 for generation\n",
    "        # Initialize the sequence with the start_sequence\n",
    "        current_input = one_hot_encode(start_sequence[0], self.input_dim).unsqueeze(0).unsqueeze(0)\n",
    "        generated_sequence = start_sequence.tolist()\n",
    "        # Generate the sequence\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Forward pass through LSTM\n",
    "            output, (state_h, state_c) = self.forward(current_input, (state_h, state_c))\n",
    "            # Get the last output (next note prediction)\n",
    "            last_output = output[:, -1, :]\n",
    "            # Apply temperature scaling and softmax to generate probabilities\n",
    "            probabilities = F.softmax(last_output / temperature, dim=1).squeeze()\n",
    "            # Sample from the probability distribution to get the next note\n",
    "            next_note = torch.multinomial(probabilities, 1).item()\n",
    "            # Append the predicted note to the sequence\n",
    "            generated_sequence.append(next_note)\n",
    "            next_input = one_hot_encode(next_note, self.input_dim).unsqueeze(0).unsqueeze(0)\n",
    "            current_input = torch.cat((current_input[:, 1:], next_input), 1)\n",
    "        return generated_sequence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cW7Hajk7UNLm"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "input_dim = good_music.max() + 1  \n",
    "\n",
    "# Convert good_music to a tensor and create the dataset\n",
    "good_music_tensor = torch.from_numpy(good_music).long()\n",
    "sequences = good_music_tensor[:, :-1]  \n",
    "targets = good_music_tensor[:, 1:]  \n",
    "dataset = TensorDataset(sequences, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Composer model\n",
    "composer = Composer(input_dim=good_music_tensor.max() + 1, lstm_units=512, num_layers=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(composer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQLKj8Xh8Vyd",
    "outputId": "b5efa979-1308-4b5f-b435-a258e65e1d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Average Loss: 5.9480\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    state_h, state_c = composer.init_state(batch_size)\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        current_batch_size = x.size(0)\n",
    "\n",
    "        # Initialize the hidden state if it's None or if the batch size has changed (last batch case)\n",
    "        if state_h is None or state_h.size(1) != current_batch_size:\n",
    "            state_h, state_c = composer.init_state(current_batch_size)\n",
    "\n",
    "        x_one_hot = nn.functional.one_hot(x, num_classes=composer.input_dim).float()\n",
    "        y_pred, (state_h, state_c) = composer(x_one_hot, (state_h, state_c))\n",
    "\n",
    "        # Flatten the output and targets\n",
    "        loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    if epoch == num_epochs - 1:\n",
    "        average_loss = total_loss / num_batches\n",
    "        print(f'Epoch {epoch}, Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5wwJFHS3OxR"
   },
   "outputs": [],
   "source": [
    "# Task3\n",
    "\n",
    "The code generates a sequence of music starting with a random note. \n",
    "This note is selected within the range of the maximum value in good_music_tensor. \n",
    "Each generated sequence by the Composer is transformed into a tensor format suitable for the Critic model. \n",
    "The Critic model then evaluates each generated sequence, providing a score that reflects the quality of the sequence as perceived by the model.\n",
    "\n",
    "Calculated the average score of all generated sequences, which was 0.5244389."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqhkuaWeZgv4",
    "outputId": "71717585-6e0d-44cb-b2fe-252a270664e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: [[0.5244389]]\n"
     ]
    }
   ],
   "source": [
    "composer = Composer(input_dim=good_music_tensor.max() + 1, lstm_units=512, num_layers=2)\n",
    "composer.eval()  \n",
    "model1 = Critic()\n",
    "model1.eval()  \n",
    "generated_sequences = []\n",
    "scores = []\n",
    "total_score = 0\n",
    "for _ in range(50):\n",
    "    start_note = random.randint(1, good_music_tensor.max())  \n",
    "    start_sequence = [start_note]\n",
    "    start_sequence_tensor = torch.tensor(start_sequence, dtype=torch.long)\n",
    "    generated_sequence = composer.compose(start_sequence_tensor, length=151)\n",
    "    generated_sequence_tensor = torch.tensor([generated_sequence], dtype=torch.float)\n",
    "    score = model1.score(generated_sequence_tensor)\n",
    "    total_score += score\n",
    "    generated_sequences.append(generated_sequence)\n",
    "    scores.append(score)\n",
    "average_score = total_score / 50\n",
    "print(f\"Average Score: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
